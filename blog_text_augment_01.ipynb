{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "blog_text_augment_01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOPFL2aTxrpv5T6pTJGFDIB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujitpal/deeplearning-ai-examples/blob/master/blog_text_augment_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtYzzydaBgUH"
      },
      "source": [
        "## Augmenting to balance unbalanced text datasets (Part 1)\n",
        "\n",
        "Idea is to investigate various techniques to augment unbalanced test datasets to balance them.\n",
        "\n",
        "We use the unbalanced [SMA Spam Collection Dataset from Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset), which has 87% ham and 13% spam SMS texts.\n",
        "\n",
        "In this notebook, we will use the [Google Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4) to generate embeddings for our sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYbhQvg2w6ek"
      },
      "source": [
        "### Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MdIdWLX_eto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9697ef-6946-4632-be16-66bd9c2fdb20"
      },
      "source": [
        "!pip install textattack"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textattack in /usr/local/lib/python3.6/dist-packages (0.2.15)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.6/dist-packages (from textattack) (3.1.0)\n",
            "Requirement already satisfied: word2number in /usr/local/lib/python3.6/dist-packages (from textattack) (1.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from textattack) (8.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from textattack) (3.0.12)\n",
            "Requirement already satisfied: language-tool-python in /usr/local/lib/python3.6/dist-packages (from textattack) (2.5.2)\n",
            "Requirement already satisfied: lemminflect in /usr/local/lib/python3.6/dist-packages (from textattack) (0.2.1)\n",
            "Requirement already satisfied: lru-dict in /usr/local/lib/python3.6/dist-packages (from textattack) (1.1.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.6/dist-packages (from textattack) (1.7.1)\n",
            "Requirement already satisfied: flair==0.6.1.post1 in /usr/local/lib/python3.6/dist-packages (from textattack) (0.6.1.post1)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from textattack) (0.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from textattack) (3.2.5)\n",
            "Requirement already satisfied: numpy<1.19.0 in /usr/local/lib/python3.6/dist-packages (from textattack) (1.18.5)\n",
            "Requirement already satisfied: bert-score>=0.3.5 in /usr/local/lib/python3.6/dist-packages (from textattack) (0.3.7)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (from textattack) (1.2.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from textattack) (1.1.5)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from textattack) (1.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from textattack) (1.7.0+cu101)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from textattack) (4.41.1)\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.6/dist-packages (from textattack) (0.5.10)\n",
            "Requirement already satisfied: transformers>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from textattack) (4.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from language-tool-python->textattack) (2.23.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (1.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (3.2.2)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (1.5.10)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (3.6.0)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (1.2.11)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (0.22.2.post1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (0.8.7)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (5.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (2.8.1)\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (0.4.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (3.6.4)\n",
            "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (4.6.2)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (2019.12.20)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (0.3.2)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (1.7.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (0.1.95)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (0.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack) (4.2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->textattack) (1.15.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets->textattack) (2.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets->textattack) (0.70.11.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets->textattack) (3.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets->textattack) (0.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets->textattack) (3.4.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets->textattack) (0.8)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.1->textattack) (2018.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->textattack) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->textattack) (0.16.0)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words->textattack) (0.6.2)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.3.0->textattack) (0.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.3.0->textattack) (20.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=3.3.0->textattack) (0.0.43)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->language-tool-python->textattack) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->language-tool-python->textattack) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->language-tool-python->textattack) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->language-tool-python->textattack) (3.0.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.6.1.post1->textattack) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.6.1.post1->textattack) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.6.1.post1->textattack) (1.3.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.6.1.post1->textattack) (4.1.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair==0.6.1.post1->textattack) (1.12.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair==0.6.1.post1->textattack) (1.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair==0.6.1.post1->textattack) (0.2.5)\n",
            "Requirement already satisfied: overrides==3.0.0 in /usr/local/lib/python3.6/dist-packages (from konoha<5.0.0,>=4.0.0->flair==0.6.1.post1->textattack) (3.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.6.1.post1->textattack) (2.5)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.6.1.post1->textattack) (3.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets->textattack) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.3.0->textattack) (7.1.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair==0.6.1.post1->textattack) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbd6qGN24T_5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwldiMjD4nBR",
        "outputId": "2a0debcc-7232-4bee-bc84-7c044d55f25f"
      },
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ApMtjoT5Djf"
      },
      "source": [
        "DATA_DIR = \"/content/drive/MyDrive/nlp-deeplearning-ai-data\"\n",
        "\n",
        "SPAM_FILE = os.path.join(DATA_DIR, \"spam.csv\")\n",
        "\n",
        "SPAM_TRAIN_FILE = os.path.join(DATA_DIR, \"spam_train.tsv\")\n",
        "SPAM_VALID_FILE = os.path.join(DATA_DIR, \"spam_valid.tsv\")\n",
        "SPAM_TEST_FILE = os.path.join(DATA_DIR, \"spam_test.tsv\")\n",
        "\n",
        "SPAM_EMB_TRAIN_FILE = os.path.join(DATA_DIR, \"spam_train_emb.tsv\")\n",
        "SPAM_EMB_VALID_FILE = os.path.join(DATA_DIR, \"spam_valid_emb.tsv\")\n",
        "SPAM_EMB_TEST_FILE = os.path.join(DATA_DIR, \"spam_test_emb.tsv\")\n",
        "\n",
        "SPAM_TA_TRAIN_FILE = os.path.join(DATA_DIR, \"spam_train_ta.tsv\")\n",
        "SPAM_TA_EMB_TRAIN_FILE = os.path.join(DATA_DIR, \"spam_train_ta_emb.tsv\")\n",
        "\n",
        "SPLIT_FRACS = [0.0, 0.7, 0.8, 1.0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnfxH4nbJwqi"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "FH_w21l55MiK",
        "outputId": "14e37f6e-6c98-4cba-f370-ecf7e81a6cad"
      },
      "source": [
        "spam_df = pd.read_csv(SPAM_FILE, encoding=\"iso-8859-1\")\n",
        "spam_df = spam_df.drop(labels=spam_df.columns[2:], axis=1)\n",
        "spam_df = spam_df.rename(columns={\"v1\": \"label\", \"v2\": \"sentence\"})\n",
        "\n",
        "spam_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                           sentence\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wztR5nEDQOA"
      },
      "source": [
        "### Class Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzhqSkOX5mB3",
        "outputId": "70b84e25-7110-4b2b-f027-9aea26bc459a"
      },
      "source": [
        "spam_df[\"label\"].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     4825\n",
              "spam     747\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "5hiCajuN7GCr",
        "outputId": "ee69a8d1-742e-471b-f9a2-74899a611393"
      },
      "source": [
        "spam_df[\"label\"].value_counts().plot(kind='bar');"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToHv9s0sLQTw"
      },
      "source": [
        "### Data Splitting\n",
        "\n",
        "Splits the input dataset into train, validation, and test sets, and writes out separate TSV files for each split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzG3f7XSJ0Pv",
        "outputId": "0809c3f4-e275-44b4-c5f7-67917f0edfe6"
      },
      "source": [
        "def split_indices(orig_df, split_fracs):\n",
        "  indices = orig_df.index.values\n",
        "  np.random.shuffle(indices)\n",
        "  splits = [int(x * len(indices)) for x in split_fracs]\n",
        "  train_indices = indices[splits[0] : splits[1]]\n",
        "  val_indices = indices[splits[1] : splits[2]]\n",
        "  test_indices = indices[splits[2]: splits[3]]\n",
        "  return train_indices, val_indices, test_indices\n",
        "\n",
        "\n",
        "train_indices, val_indices, test_indices = split_indices(spam_df, SPLIT_FRACS)\n",
        "len(train_indices), len(val_indices), len(test_indices)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3900, 557, 1115)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZFUQ5V1Wyhj",
        "outputId": "975a7af4-caf5-445f-c47f-b8b767b379e0"
      },
      "source": [
        "spam_train_df = spam_df.iloc[train_indices]\n",
        "spam_valid_df = spam_df.iloc[val_indices]\n",
        "spam_test_df = spam_df.iloc[test_indices]\n",
        "\n",
        "len(spam_train_df), len(spam_valid_df), len(spam_test_df)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3900, 557, 1115)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKZc8vrOYJq6"
      },
      "source": [
        "spam_train_df.to_csv(SPAM_TRAIN_FILE, sep='\\t')\n",
        "spam_valid_df.to_csv(SPAM_VALID_FILE, sep='\\t')\n",
        "spam_test_df.to_csv(SPAM_TEST_FILE)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N2U4FM5DUZL"
      },
      "source": [
        "### Compute Sentence Embeddings\n",
        "\n",
        "For each data split, we compute embeddings using the Google Universal Sentence Encoder (GUSE)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EDzrqiH4bS-",
        "outputId": "35dec450-9f45-4190-aade-4fa69851ff57"
      },
      "source": [
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "embeddings = embed([\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"I am a sentence for which I would like to get its embedding\"])\n",
        "\n",
        "len(embeddings), embeddings[0].numpy().shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, (512,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6YnXCaNYfEP"
      },
      "source": [
        "def compute_embeddings_and_write(input_df, model, output_file):\n",
        "  if os.path.exists(output_file):\n",
        "    print(\"Output file: {:s} already exists, skipping embedding\".format(output_file))\n",
        "    return\n",
        "  fout = open(output_file, \"w\")\n",
        "  fout.write(\"sentence\\tlabel\\tembedding\\n\")\n",
        "  batch_size = 128\n",
        "\n",
        "  num_recs = len(input_df)\n",
        "  start, stop = 0, 0\n",
        "  num_batches = (num_recs // batch_size) + 1\n",
        "\n",
        "  for i in range(num_batches):\n",
        "    stop = min(start + batch_size, num_recs)\n",
        "    batch_df = input_df.iloc[start : stop]\n",
        "    batch_labels = batch_df[\"label\"].values\n",
        "    batch_sents = batch_df[\"sentence\"].values\n",
        "    batch_embeddings = model(batch_sents)\n",
        "    for label, sent, embedding in zip(batch_labels, batch_sents, batch_embeddings):\n",
        "      embed_str = \",\".join([\"{:.7e}\".format(x) for x in embedding.numpy().tolist()])\n",
        "      fout.write(\"{:s}\\t{:s}\\t{:s}\\n\".format(sent, label, str(embed_str)))\n",
        "    start = stop\n",
        "\n",
        "  fout.close()\n",
        "\n",
        "\n",
        "compute_embeddings_and_write(spam_train_df, embed, SPAM_EMB_TRAIN_FILE)\n",
        "compute_embeddings_and_write(spam_valid_df, embed, SPAM_EMB_VALID_FILE)\n",
        "compute_embeddings_and_write(spam_test_df, embed, SPAM_EMB_TEST_FILE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK0DUmngwKrn"
      },
      "source": [
        "## Augment using TextAttack\n",
        "\n",
        "We augment the text for the training split only. In order to balance the dataset, for each `spam` record, we want to augment it with N additional variants provided by TextAttack, where the `ham` class is (N+1) times more common than `spam`.\n",
        "\n",
        "**WARNING: This step takes a very long time to complete!**\n",
        "\n",
        "We then compute and write out embeddings for the augmented training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO350b94wOXO"
      },
      "source": [
        "from textattack.transformations import WordSwapRandomCharacterDeletion\n",
        "from textattack.transformations import WordSwapQWERTY\n",
        "from textattack.transformations import CompositeTransformation\n",
        "\n",
        "from textattack.constraints.pre_transformation import RepeatModification\n",
        "from textattack.constraints.pre_transformation import StopwordModification\n",
        "\n",
        "from textattack.augmentation import Augmenter\n",
        "from textattack.augmentation import CheckListAugmenter\n",
        "from textattack.augmentation import WordNetAugmenter"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w335XXinxWGh",
        "outputId": "15f7d0e6-eaa8-4c4e-f677-c2ded734a29d"
      },
      "source": [
        "num_major = len(spam_train_df[spam_train_df[\"label\"] == \"ham\"])\n",
        "num_minor = len(spam_train_df[spam_train_df[\"label\"] == \"spam\"])\n",
        "\n",
        "num_augmentations = int(num_major / num_minor)\n",
        "num_major, num_minor, num_augmentations"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3355, 545, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixongt7j0L5-"
      },
      "source": [
        "augmenters = []\n",
        "# augment using mis-spellings\n",
        "transformation = CompositeTransformation(\n",
        "    [WordSwapRandomCharacterDeletion(), WordSwapQWERTY()])\n",
        "# Set up constraints\n",
        "constraints = [RepeatModification(), StopwordModification()]\n",
        "# Create augmenter with specified parameters\n",
        "augmenter = Augmenter(transformation=transformation, \n",
        "                      constraints=constraints, \n",
        "                      pct_words_to_swap=0.5, \n",
        "                      transformations_per_example=1)\n",
        "augmenters.append(augmenter)\n",
        "# augment using checklists\n",
        "augmenter = CheckListAugmenter(pct_words_to_swap=0.2, \n",
        "                               transformations_per_example=2)\n",
        "augmenters.append(augmenter)\n",
        "# augment using wordnet\n",
        "augmenter = WordNetAugmenter(pct_words_to_swap=0.2, \n",
        "                             transformations_per_example=2)\n",
        "augmenters.append(augmenter)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxe85qGCyTX5",
        "outputId": "b85b340d-1b91-4349-cba0-341f31cda5c2"
      },
      "source": [
        "def augment_sentence(sentence, augmenters):\n",
        "  aug_sents = []\n",
        "  for augmenter in augmenters:\n",
        "    try:\n",
        "      sentences = augmenter.augment(sentence)\n",
        "      aug_sents.extend(sentences)\n",
        "    except IndexError:\n",
        "      continue\n",
        "  return aug_sents\n",
        "\n",
        "# self-test\n",
        "augment_sentence(spam_train_df.iloc[0][\"sentence\"], augmenters)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-01 20:25:49,797 loading file /root/.flair/models/en-ner-conll03-v0.4.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Ugh I do not wanna get out of bed. It's so warm.\",\n",
              " \"Ugh I don't wanna get out of bed. It's so warm.\",\n",
              " \"Ugh I don't wanna commence out of bed. It's so lovesome.\",\n",
              " \"Ugh ace don't wanna gravel out of bed. It's so warm.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7Q5By6Qa1zJ",
        "outputId": "a635c8c2-8794-445b-81ef-0a477955ca49"
      },
      "source": [
        "if os.path.exists(SPAM_TA_TRAIN_FILE):\n",
        "  print(\"TextAttack augmented Spam Training file ({:s}) already exists, skipping\")\n",
        "else:\n",
        "  fout = open(SPAM_TA_TRAIN_FILE, \"w\")\n",
        "  fout.write(\"sentence\\tlabel\\n\")\n",
        "\n",
        "  num_processed, num_augmented = 0, 0\n",
        "  for _, row in spam_train_df.iterrows():\n",
        "    if num_processed % 1000 == 0:\n",
        "      print(\"{:d} sentences processed, {:d} augmented\"\n",
        "        .format(num_processed, num_augmented))\n",
        "    label = row.label\n",
        "    sentence = row.sentence\n",
        "    if label == \"ham\":\n",
        "      fout.write(\"{:s}\\t{:s}\\n\".format(sentence, label))\n",
        "    else:\n",
        "      # write out original sentence\n",
        "      fout.write(\"{:s}\\t{:s}\\n\".format(sentence, label))\n",
        "      # augment and write out augmented sentences\n",
        "      augmented_sents = augment_sentence(sentence, augmenters)\n",
        "      for sent in augmented_sents:\n",
        "        fout.write(\"{:s}\\t{:s}\\n\".format(sent, label))\n",
        "      num_augmented += 1\n",
        "    num_processed += 1\n",
        "\n",
        "  print(\"{:d} sentences processed, {:d} augmented, COMPLETE\"\n",
        "    .format(num_processed, num_augmented))\n",
        "  fout.close()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 sentences processed, 0 augmented\n",
            "1000 sentences processed, 126 augmented\n",
            "2000 sentences processed, 267 augmented\n",
            "3000 sentences processed, 421 augmented\n",
            "3900 sentences processed, 545 augmented, COMPLETE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h85-VX7b7xw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "02c45104-8998-4ee4-d11e-003fee120e5e"
      },
      "source": [
        "spam_train_ta_df = pd.read_csv(SPAM_TA_TRAIN_FILE, sep='\\t')\n",
        "spam_train_ta_df = spam_train_ta_df.dropna()\n",
        "spam_train_ta_df = spam_train_ta_df.reset_index()\n",
        "spam_train_ta_df.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Ugh I don't wanna get out of bed. It's so warm.</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Dear i have reache room</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I always chat with you. In fact i need money c...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Yes just finished watching days of our lives. ...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Nice.nice.how is it working?</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index                                           sentence label\n",
              "0      0    Ugh I don't wanna get out of bed. It's so warm.   ham\n",
              "1      1                            Dear i have reache room   ham\n",
              "2      2  I always chat with you. In fact i need money c...   ham\n",
              "3      3  Yes just finished watching days of our lives. ...   ham\n",
              "4      4                       Nice.nice.how is it working?   ham"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f86aEybFd06N"
      },
      "source": [
        "compute_embeddings_and_write(spam_train_ta_df, embed, SPAM_TA_EMB_TRAIN_FILE)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7MTS2aqeNYE"
      },
      "source": [
        "### Clean Up (After experiment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYwJe1JG4DRq"
      },
      "source": [
        "# temp_files = [\n",
        "#   SPAM_TRAIN_FILE,\n",
        "#   SPAM_VALID_FILE,\n",
        "#   SPAM_TEST_FILE,\n",
        "#   SPAM_EMB_TRAIN_FILE,\n",
        "#   SPAM_EMB_VALID_FILE,\n",
        "#   SPAM_EMB_TEST_FILE,\n",
        "#   SPAM_TA_TRAIN_FILE,\n",
        "#   SPAM_TA_EMB_TRAIN_FILE\n",
        "# ]\n",
        "# for temp_file in temp_files:\n",
        "#   if os.path.exists(temp_file):\n",
        "#     os.unlink(temp_file)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfuXDcJaeuzY"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}