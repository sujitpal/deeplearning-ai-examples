{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lf1-longformer-pretrained",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVwXhpMK6LuGzxrJaUIhci",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujitpal/nlp-deeplearning-ai-examples/blob/master/lf1_longformer_pretrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQbH2PzNTyWX",
        "colab_type": "text"
      },
      "source": [
        "## LongTransformer Tests (Pretrained)\n",
        "\n",
        "This notebook covers two use cases using a pre-trained [LongFormer](https://arxiv.org/abs/2004.05150) model. The first use case uses a pre-trained LongFormer language model to generate document embeddings, and the second use case uses a LongFormer model [fine-tuned on the SQuAD v1 dataset](https://huggingface.co/valhalla/longformer-base-4096-finetuned-squadv1) for question answering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2aqZZNE8Et1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "b830e9b0-72a2-413a-ea53-cb452d1e7996"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT_RIAveT3RO",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgmcFxdY8B7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    LongformerModel, LongformerTokenizer, LongformerForSequenceClassification,\n",
        "    TFLongformerForQuestionAnswering, LongformerForMultipleChoice)\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3728E2zqT9Fw",
        "colab_type": "text"
      },
      "source": [
        "### Data\n",
        "\n",
        "Copied first paragraph of 3 wikipedia pages on Albert Einstein, Richard Feynman, and William Shakespeare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7WGepOZ8soe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cfc27a3b-ca6a-4187-8980-8bbaeafd88be"
      },
      "source": [
        "einstein_text = \"\"\"Albert Einstein; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist[5] who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics).[3][6]:274 His work is also known for its influence on the philosophy of science.[7][8] He is best known to the general public for his mass–energy equivalence formula E = mc2, which has been dubbed \"the world's most famous equation\".[9] He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\",[10] a pivotal step in the development of quantum theory.\"\"\"\n",
        "feynman_text = \"\"\"Richard Phillips Feynman ForMemRS (/ˈfaɪnmən/; May 11, 1918 – February 15, 1988) was an American theoretical physicist, known for his work in the path integral formulation of quantum mechanics, the theory of quantum electrodynamics, the physics of the superfluidity of supercooled liquid helium, as well as his work in particle physics for which he proposed the parton model. For contributions to the development of quantum electrodynamics, Feynman received the Nobel Prize in Physics in 1965 jointly with Julian Schwinger and Shin'ichirō Tomonaga.\"\"\"\n",
        "shakespeare_text = \"\"\"William Shakespeare (bapt. 26 April 1564 – 23 April 1616)[a] was an English playwright, poet, and actor, widely regarded as the greatest writer in the English language and the world's greatest dramatist.[2][3][4] He is often called England's national poet and the \"Bard of Avon\" (or simply \"the Bard\").[5][b] His extant works, including collaborations, consist of some 39 plays,[c] 154 sonnets, two long narrative poems, and a few other verses, some of uncertain authorship. His plays have been translated into every major living language and are performed more often than those of any other playwright.[7] They also continue to be studied and reinterpreted.\"\"\"\n",
        "\n",
        "len(einstein_text), len(feynman_text), len(shakespeare_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(655, 548, 658)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqHRzl8tUNot",
        "colab_type": "text"
      },
      "source": [
        "### Instantiate Pretrained Longformer model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BhO2Z9b7eZx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "43eebac6-ec51-4705-cee4-484b6755c40a"
      },
      "source": [
        "model = LongformerModel.from_pretrained('allenai/longformer-base-4096', return_dict=True)\n",
        "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of LongformerModel were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['longformer.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2MwyaVRUU1v",
        "colab_type": "text"
      },
      "source": [
        "### Document Embeddings\n",
        "\n",
        "We tried using the following embedding strategies. \n",
        "\n",
        "* Getting the embedding for the document via the `pooler_output` (which is the [CLS] followed by a tanh according to the [Longformer docs](https://huggingface.co/transformers/model_doc/longformer.html)).\n",
        "* Getting the last hidden state via the `last_hidden_state` for the document and then computing the mean across all elements in the sequence.\n",
        "\n",
        "Idea is that cosine similarity between embeddings for Einstein and Feynman should be higher than cosine similarity between Einstein and Shakespeare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZdOvb4xVvpi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "da8c39d1-6c9d-4edd-8176-52bc0351ee45"
      },
      "source": [
        "def get_cls_embedding(text, tokenizer, model, max_length=4096):\n",
        "  input_ids = torch.tensor(tokenizer.encode(text)[0:max_length]).unsqueeze(0)\n",
        "  attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
        "\n",
        "  outputs = model(input_ids, attention_mask=attention_mask)\n",
        "  text_embedding = outputs.pooler_output\n",
        "\n",
        "  return text_embedding.detach().numpy()\n",
        "\n",
        "einstein_embedding = get_cls_embedding(einstein_text, tokenizer, model)\n",
        "feynman_embedding = get_cls_embedding(feynman_text, tokenizer, model)\n",
        "shakespeare_embedding = get_cls_embedding(shakespeare_text, tokenizer, model)\n",
        "\n",
        "X = np.array([einstein_embedding, feynman_embedding, shakespeare_embedding]).squeeze(axis=1)\n",
        "print(cosine_similarity(X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.99713266 0.9979736 ]\n",
            " [0.99713266 1.0000001  0.9953785 ]\n",
            " [0.9979736  0.9953785  0.9999999 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knVGfanfWnxI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1001d0cf-dd06-4b21-e05c-5e1aa8060db1"
      },
      "source": [
        "def get_mean_embedding(text, tokenizer, model, max_length=4096):\n",
        "  input_ids = torch.tensor(tokenizer.encode(text)[0:max_length]).unsqueeze(0)\n",
        "  attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
        "\n",
        "  outputs = model(input_ids, attention_mask=attention_mask)\n",
        "  text_embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "\n",
        "  return text_embedding.detach().numpy()\n",
        "\n",
        "einstein_embedding = get_mean_embedding(einstein_text, tokenizer, model)\n",
        "feynman_embedding = get_mean_embedding(feynman_text, tokenizer, model)\n",
        "shakespeare_embedding = get_mean_embedding(shakespeare_text, tokenizer, model)\n",
        "\n",
        "X = np.array([einstein_embedding, feynman_embedding, shakespeare_embedding]).squeeze(axis=1)\n",
        "print(cosine_similarity(X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.9999999 0.990968  0.987079 ]\n",
            " [0.990968  1.0000002 0.981339 ]\n",
            " [0.987079  0.981339  1.0000004]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixiJ6k2mXnGm",
        "colab_type": "text"
      },
      "source": [
        "### Trivia Question Answering\n",
        "\n",
        "This model has been fine-tuned for trivia style question answering using the [SQuAD v1 dataset](https://rajpurkar.github.io/SQuAD-explorer/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0fge2zNb-sg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "fb084e5d-7386-45ee-f704-ff17403cd341"
      },
      "source": [
        "tokenizer = LongformerTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
        "model = TFLongformerForQuestionAnswering.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing TFLongformerForQuestionAnswering.\n",
            "\n",
            "All the weights of TFLongformerForQuestionAnswering were initialized from the model checkpoint at valhalla/longformer-base-4096-finetuned-squadv1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerForQuestionAnswering for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQwz8O4zXxYv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "07744340-817d-4a2f-f50f-442612c0ddbf"
      },
      "source": [
        "def answer_question(question, passage, model, tokenizer):\n",
        "  input_dict = tokenizer(question, passage, return_tensors='tf')\n",
        "  start_scores, end_scores = model(input_dict)\n",
        "  all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n",
        "  answer = ' '.join(all_tokens[tf.math.argmax(start_scores, 1)[0] : tf.math.argmax(end_scores, 1)[0]+1])\n",
        "  return answer\n",
        "\n",
        "answer = answer_question(\"Who was Albert Einstein?\", einstein_text, model, tokenizer)\n",
        "print(\"answer:\", answer)\n",
        "answer = answer_question(\"When was William Shakespeare born?\", shakespeare_text, model, tokenizer)\n",
        "print(\"answer:\", answer)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "answer: Ġa ĠGerman - born Ġtheoretical Ġphysicist\n",
            "answer: Ġ26 ĠApril Ġ15 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccxGEvtatUqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}