{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_04_word_translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4un4GSS0Zi7zVe7uiIRkQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujitpal/nlp-deeplearning-ai-examples/blob/master/01_04_word_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmIwM9UUdznQ",
        "colab_type": "text"
      },
      "source": [
        "#English to Spanish Word Translation\n",
        "\n",
        "Idea is to learn the transformation (as a set of weights for a neural network) from embeddings of a word in English to the embeddings of the equivalent word in Spanish. We use the [FreeDict site](https://freedict.org/downloads/#dictionary-downloads) to get the set of parallel words.\n",
        "\n",
        "(Note: the downloaded file eng-spa.dict.dz file is in the form of plain text and is not convenient for machine reading, better to download [the TEI (XML) file from github](https://github.com/freedict/fd-dictionaries/blob/master/eng-spa/eng-spa.tei) and parse as shown here.)\n",
        "\n",
        "To generate the embeddings, we use Fasttext which provides English and Spanish embeddings.\n",
        "\n",
        "We then train a neural network to take as input the english embedding for a word and output the corresponding Spanish embedding.\n",
        "\n",
        "We also train a KD-Tree on the Spanish embeddings of the test set, so we can find nearest neighbors of the predictions.\n",
        "\n",
        "During prediction, we send in the embeddings for the English word, and check the top 3 nearest neighbors of the predicted embedding for the Spanish word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5H_71O6pRcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "63439e88-94b5-4478-bada-71b57b0fed6f"
      },
      "source": [
        "%pip install fasttext"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 20.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 24.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 14.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 14.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (49.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3015765 sha256=16dcc59e94c4ba1b839962e9f391aea61ee48fd28466bcfb28db67e1278121d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc2nYmDKdhvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KDTree\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3w3FjpzjW8T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "528715ed-67a0-47cb-e2f5-fbf931b23082"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive # import drive from google colab\n",
        "\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1eKO9iTj0JL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d1efa355-48d5-4d67-ed94-61466137be93"
      },
      "source": [
        "%ls \"drive/My Drive/nlp-deeplearning-ai-data\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cc.en.300.bin   eng-spa.tei     testdata.manual.2009.06.14.csv\n",
            "cc.es.300.bin   eng-spa.tsv     training.1600000.processed.noemoticon.csv\n",
            "eng-embeds.txt  spa_embeds.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xUzQxeuj8P6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = \"drive/My Drive/nlp-deeplearning-ai-data\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IByVqWeYOu4X",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Creation\n",
        "\n",
        "Get data from FreeDict for parallel english-spanish word set, then generate embeddings using the Fasttext English and Spanish models respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ter3hIb4kSV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maybe_parse_xmldict(tei_file, tsv_file):\n",
        "  if not os.path.exists(tsv_file):\n",
        "    ftsv = open(tsv_file, \"w\")\n",
        "    with open(tei_file) as f:\n",
        "      contents = f.read()\n",
        "    num_pairs = 0\n",
        "    soup = BeautifulSoup(contents, \"xml\")\n",
        "    for entry in soup.find_all(\"entry\"):\n",
        "      if num_pairs % 1000 == 0:\n",
        "        print(\"{:d} pairs extracted\".format(num_pairs))\n",
        "      eng = entry.find_all(\"orth\")[0].get_text()\n",
        "      spa = entry.find_all(\"quote\")[0].get_text()\n",
        "      ftsv.write(\"{:s}\\t{:s}\\n\".format(eng, spa))\n",
        "      num_pairs += 1\n",
        "\n",
        "    print(\"{:d} pairs extracted, COMPLETE\".format(num_pairs))\n",
        "    ftsv.close()\n",
        "\n",
        "\n",
        "maybe_parse_xmldict(os.path.join(DATA_DIR, \"eng-spa.tei\"),\n",
        "                    os.path.join(DATA_DIR, \"eng-spa.tsv\"))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzuJiovSlEOL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a34c4fa6-3ec1-444f-cf71-1c700812c86d"
      },
      "source": [
        "words_df = pd.read_csv(os.path.join(DATA_DIR, \"eng-spa.tsv\"), sep='\\t', names=[\"eng\", \"spa\"])\n",
        "words_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>spa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zucchini</td>\n",
              "      <td>calabacín</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ABC</td>\n",
              "      <td>abecé</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Adam</td>\n",
              "      <td>Adán</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Africa</td>\n",
              "      <td>África</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>African</td>\n",
              "      <td>africano</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        eng        spa\n",
              "0  zucchini  calabacín\n",
              "1       ABC      abecé\n",
              "2      Adam       Adán\n",
              "3    Africa     África\n",
              "4   African   africano"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO47BvKdvOGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maybe_generate_embeddings(words, lang, model_name, embed_file):\n",
        "  if not os.path.exists(embed_file):\n",
        "    fasttext.util.download_model(lang, if_exists=\"ignore\")\n",
        "    embeddings = fasttext.load_model(model_name)\n",
        "    femb = open(embed_file, \"w\")\n",
        "    for word in words:\n",
        "      vec = embeddings[word]\n",
        "      vec_str = \" \".join([\"{:.5f}\".format(v) for v in vec])\n",
        "      femb.write(\"{:s}\\t{:s}\\n\".format(word, vec_str))\n",
        "    femb.close()\n",
        "\n",
        "maybe_generate_embeddings(words_df[\"eng\"].values, \"en\", \"cc.en.300.bin\",\n",
        "                          os.path.join(DATA_DIR, \"eng-embeds.txt\"))\n",
        "maybe_generate_embeddings(words_df[\"spa\"].values, \"es\", \"cc.es.300.bin\",\n",
        "                          os.path.join(DATA_DIR, \"spa_embeds.txt\"))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnfv1AzMPs2d",
        "colab_type": "text"
      },
      "source": [
        "## Split into Train, Validation, and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dhx89Dci9go",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2572fc4-de5f-4e4e-b861-ff4def3a9c79"
      },
      "source": [
        "def extract_vectors(eng_file, spa_file):\n",
        "    eng_words, eng_vecs, spa_words, spa_vecs = [], [], [], []\n",
        "    with open(eng_file, \"r\") as feng:\n",
        "      for line in feng:\n",
        "        word, vec = line.strip().split('\\t')\n",
        "        eng_words.append(word)\n",
        "        eng_vecs.append([float(v) for v in vec.split()])\n",
        "    with open(spa_file, \"r\") as fspa:\n",
        "      for line in fspa:\n",
        "        word, vec = line.strip().split('\\t')\n",
        "        spa_words.append(word)\n",
        "        spa_vecs.append([float(v) for v in vec.split()])\n",
        "    return eng_words, np.array(eng_vecs), spa_words, np.array(spa_vecs)  \n",
        "\n",
        "eng_words, X, spa_words, Y = extract_vectors(\n",
        "    os.path.join(DATA_DIR, \"eng-embeds.txt\"),\n",
        "    os.path.join(DATA_DIR, \"spa_embeds.txt\"))\n",
        "\n",
        "Xtv, Xtest, Ytv, Ytest, _, etest, _, stest = train_test_split(X, Y, eng_words, spa_words, test_size=0.3)\n",
        "Xtrain, Xval, Ytrain, Yval = train_test_split(Xtv, Ytv, test_size=0.1)\n",
        "Xtrain.shape, Ytrain.shape, Xval.shape, Yval.shape, Xtest.shape, Ytest.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3720, 300), (3720, 300), (414, 300), (414, 300), (1773, 300), (1773, 300))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAn_njt_Pxzj",
        "colab_type": "text"
      },
      "source": [
        "## Torch Stuff -- Dataset, DataLoader, and Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exRm3nNG8TzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_dataset(X, Y):\n",
        "  xt = torch.tensor(X, dtype=torch.float32)\n",
        "  yt = torch.tensor(Y, dtype=torch.float32)\n",
        "  return TensorDataset(xt, yt)\n",
        "\n",
        "train_ds = make_dataset(Xtrain, Ytrain)\n",
        "val_ds = make_dataset(Xval, Yval)\n",
        "test_ds = make_dataset(Xtest, Ytest)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4J9-wTEinmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
        "test_dl = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5GD3_5XmG5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a9f73850-0e3e-40cf-db77-4d3f06a62813"
      },
      "source": [
        "class WordTranslationNet(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.encoder = nn.Linear(input_size, hidden_size)\n",
        "    self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.decoder(x)\n",
        "    return x\n",
        "\n",
        "net = WordTranslationNet(300, 50, 300)\n",
        "net"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordTranslationNet(\n",
              "  (encoder): Linear(in_features=300, out_features=50, bias=True)\n",
              "  (decoder): Linear(in_features=50, out_features=300, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9ihjBJIP5ML",
        "colab_type": "text"
      },
      "source": [
        "## Train and Eval loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rM2xbZLm8ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, dev, train_dl, val_dl, num_epochs=20, lr=1e-4):\n",
        "  params = filter(lambda p: p.requires_grad, net.parameters())\n",
        "  optimizer = torch.optim.Adam(params, lr=lr)\n",
        "  for i in range(num_epochs):\n",
        "    net.train()\n",
        "    sum_loss, total = 0, 0\n",
        "    for x, y in train_dl:\n",
        "      x, y = x.to(dev), y.to(dev)\n",
        "      y_ = net(x)\n",
        "      optimizer.zero_grad()\n",
        "      loss = F.mse_loss(y_, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      sum_loss += loss.item() * y.shape[0]\n",
        "      total += y.shape[0]\n",
        "    val_loss = evaluate(net, dev, val_dl)\n",
        "    print(\"EPOCH {:d}: train loss: {:.3f}, val loss: {:.3f}\"\n",
        "      .format(i, sum_loss / total, val_loss))\n",
        "\n",
        "\n",
        "def evaluate(net, dev, val_dl):\n",
        "  net.eval()\n",
        "  correct, total, sum_loss = 0, 0, 0\n",
        "  for x, y in val_dl:\n",
        "    x, y = x.to(dev), y.to(dev)\n",
        "    y_ = net(x)\n",
        "    loss = F.mse_loss(y_, y)\n",
        "    _, pred = torch.max(y_, 1)\n",
        "    total += y.shape[0]\n",
        "    sum_loss += loss.item() * y.shape[0]\n",
        "  return sum_loss / total"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UQlNhxZP91m",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nqgxDC7oC-A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2c2e5cb9-17a6-455e-cc95-1e98506bf362"
      },
      "source": [
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(dev)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordTranslationNet(\n",
              "  (encoder): Linear(in_features=300, out_features=50, bias=True)\n",
              "  (decoder): Linear(in_features=50, out_features=300, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_nIRifToIJU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "635ce72d-9ca6-4caf-feb3-fb9fe5f2651e"
      },
      "source": [
        "train(net, dev, train_dl, val_dl, num_epochs=100)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0: train loss: 0.009, val loss: 0.007\n",
            "EPOCH 1: train loss: 0.005, val loss: 0.004\n",
            "EPOCH 2: train loss: 0.003, val loss: 0.003\n",
            "EPOCH 3: train loss: 0.003, val loss: 0.003\n",
            "EPOCH 4: train loss: 0.003, val loss: 0.003\n",
            "EPOCH 5: train loss: 0.003, val loss: 0.003\n",
            "EPOCH 6: train loss: 0.003, val loss: 0.002\n",
            "EPOCH 7: train loss: 0.003, val loss: 0.002\n",
            "EPOCH 8: train loss: 0.003, val loss: 0.002\n",
            "EPOCH 9: train loss: 0.003, val loss: 0.002\n",
            "EPOCH 10: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 11: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 12: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 13: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 14: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 15: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 16: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 17: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 18: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 19: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 20: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 21: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 22: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 23: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 24: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 25: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 26: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 27: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 28: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 29: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 30: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 31: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 32: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 33: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 34: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 35: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 36: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 37: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 38: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 39: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 40: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 41: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 42: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 43: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 44: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 45: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 46: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 47: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 48: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 49: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 50: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 51: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 52: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 53: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 54: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 55: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 56: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 57: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 58: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 59: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 60: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 61: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 62: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 63: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 64: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 65: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 66: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 67: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 68: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 69: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 70: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 71: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 72: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 73: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 74: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 75: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 76: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 77: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 78: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 79: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 80: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 81: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 82: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 83: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 84: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 85: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 86: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 87: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 88: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 89: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 90: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 91: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 92: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 93: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 94: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 95: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 96: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 97: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 98: train loss: 0.002, val loss: 0.002\n",
            "EPOCH 99: train loss: 0.002, val loss: 0.002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq1BF2VoQA6f",
        "colab_type": "text"
      },
      "source": [
        "## Generate Predictions and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f49gDSq9oJC7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52cd78e0-9f5f-4a44-e19f-4874f2ba5d56"
      },
      "source": [
        "labels, predictions = [], []\n",
        "net.eval()\n",
        "for xtest, ytest in test_dl:\n",
        "  xtest = xtest.to(dev)\n",
        "  ytest_ = net(xtest)\n",
        "  # labels.extend(ytest.numpy())\n",
        "  predictions.extend(ytest_.cpu().detach().numpy())\n",
        "\n",
        "len(labels), len(predictions)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 1773)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp2I9VH5sU4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93cb7981-e1e0-4101-b1b1-b8d44fb4d403"
      },
      "source": [
        "kdt = KDTree(Ytest, leaf_size=30, metric='euclidean')\n",
        "kdt.query(Ytest[0,:].reshape(1, -1), k=3, return_distance=False)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0, 1172, 1255]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9xumx68twXw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "outputId": "05ee2836-0417-4ea4-929e-e9a6d5f8c710"
      },
      "source": [
        "correct, incorrect = 0, 0\n",
        "for i, pred in enumerate(predictions):\n",
        "  neighbors = kdt.query(pred.reshape(1, -1), k=3, return_distance=False)[0]\n",
        "  if i in neighbors:\n",
        "    print(i, etest[i], stest[neighbors[0]], stest[neighbors[1]], stest[neighbors[2]])\n",
        "    correct += 1\n",
        "  else:\n",
        "    incorrect += 1\n",
        "print(correct, incorrect)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "198 wife aristócrata esposa periodista\n",
            "202 milk leche mermelada chocolate\n",
            "211 fuel tank depósito de gasolina depósito de gasolina de poca profundidad\n",
            "235 Greenland Escandinavia Groenlandia Mediterráneo\n",
            "255 knife cuchillo camisadedeporte ensangrentado\n",
            "260 destroy controlar considerar destruir\n",
            "298 veal mantequilla berenjena carnedeternera\n",
            "304 warm caliente caliente agradable\n",
            "334 also de poca profundidad también también\n",
            "335 conquer abandonar aprovechar conquistar\n",
            "405 beet remolacha mantequilla chocolate\n",
            "412 zoo civilización jardínzoológico billetedeidayvuelta\n",
            "533 consider desentenderse de considerar desprendimiento de tierras\n",
            "535 petrol tank depósito de gasolina depósito de gasolina piernas torcidas hacia afuera\n",
            "626 baker panadero aristócrata periodista\n",
            "647 suitable conveniente conveniente desentenderse de\n",
            "680 Asian americano asiático asiático\n",
            "684 sustain considerar aprovechar sostener\n",
            "689 too demasiado sobretodo bastante\n",
            "751 apricot albaricoquero depósito de gasolina depósito de gasolina\n",
            "758 naughty muchachotravieso depósito de gasolina depósito de gasolina\n",
            "806 crab cangrejo langosta elefante\n",
            "826 hot caliente caliente líquido\n",
            "888 lovely agradable agradable agradable\n",
            "925 ashen ensangrentado ceniciento horrible\n",
            "928 occurence acontecimiento acontecimiento acontecimiento\n",
            "954 Mrs señorita malacostumbre Blanca‐Nieve\n",
            "1006 her ella ella porque\n",
            "1074 every toda clase de toda clase de toda clase de\n",
            "1093 succeed desentenderse de considerar conseguir\n",
            "1097 Denmark Escandinavia Dinamarca GranBretaña\n",
            "1106 Brussels coles de Bruselas GranBretaña Gran Bretaña\n",
            "1136 all toda clase de toda clase de toda clase de\n",
            "1231 nice agradable agradable agradable\n",
            "1282 all toda clase de toda clase de toda clase de\n",
            "1301 she ella ella alguien\n",
            "1359 Scandinavia Escandinavia depósito de gasolina depósito de gasolina\n",
            "1380 bright magnífico muchachotravieso papelhigiénico\n",
            "1417 sir caballero caballero también\n",
            "1434 dawn amanecer mañanaporlamañana billetedeidayvuelta\n",
            "1460 tea tetera bebida tilo\n",
            "1478 enjoy disfrutar considerar aprovechar\n",
            "1498 danger of skidding depósito de gasolina depósito de gasolina carretera resbaladiza\n",
            "1504 chase perseguir perseguir perseguir\n",
            "1533 raven cuervo elefante hipopótamo\n",
            "1592 trouble desentenderse de inconveniente inconveniente\n",
            "1633 pond estanque carretera resbaladiza camisadedeporte\n",
            "1653 tar alquitrán desconcharse papelhigiénico\n",
            "1664 nor tampoco porque considerar\n",
            "1678 event acontecimiento acontecimiento acontecimiento\n",
            "1698 nylon papelhigiénico billetedeidayvuelta de nilón\n",
            "1699 flake desconcharse carretera resbaladiza mantequilla\n",
            "1759 boar jabalí cerdo elefante\n",
            "1761 ale chocolate cerveza mermelada\n",
            "1764 fit sostener acomodar acomodar\n",
            "55 1718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQk0-r50MxGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}