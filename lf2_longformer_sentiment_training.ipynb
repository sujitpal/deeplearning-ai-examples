{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lf2-longformer-sentiment-training",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwBDjyp8KDerDv1S9sUcS7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujitpal/nlp-deeplearning-ai-examples/blob/master/lf2_longformer_sentiment_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQbH2PzNTyWX"
      },
      "source": [
        "## LongTransformer Tests (Fine tuning)\n",
        "\n",
        "In this notebook, we will fine tune a `LongformerForSequenceClassification` model to do sentiment analysis. Our dataset will come from the [Sentiment Labeled dataset from the UCI ML repository](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ4s2UxW_qen"
      },
      "source": [
        "### Machine Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2aqZZNE8Et1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b151e7b-4c52-4140-a3c5-ad79e4527131"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul0fr8dQkn1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115b129c-98eb-4cba-e316-f1845772f390"
      },
      "source": [
        "%%bash\n",
        "(\n",
        "if [ -n \"sentiment labelled sentences.zip\" ]\n",
        "then\n",
        "  wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\"\n",
        "  unzip -a \"sentiment labelled sentences.zip\"\n",
        "fi\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  sentiment labelled sentences.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2020-12-29 03:32:56--  https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84188 (82K) [application/x-httpd-php]\n",
            "Saving to: ‘sentiment labelled sentences.zip.1’\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 60%  871K 0s\n",
            "    50K .......... .......... .......... ..                   100% 78.5M=0.06s\n",
            "\n",
            "2020-12-29 03:32:56 (1.39 MB/s) - ‘sentiment labelled sentences.zip.1’ saved [84188/84188]\n",
            "\n",
            "replace sentiment labelled sentences/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
            "(EOF or read error, treating as \"[N]one\" ...)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cf-wQiw_lLX"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOpZkmvtm0Gs"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from torch.nn import functional as F\n",
        "from transformers import (\n",
        "    LongformerTokenizer, LongformerForSequenceClassification,\n",
        "    AdamW\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1s5UW1B_nG4"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvEo9SuDmMYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eef9632-fc08-444e-f8c6-f3d651c4b4ce"
      },
      "source": [
        "def load_sentences_and_labels(folder):\n",
        "  sentences, labels = [], []\n",
        "  for labelled_file in os.listdir(folder):\n",
        "    if labelled_file.split('.')[0].endswith(\"_labelled\"):\n",
        "      flab = open(os.path.join(folder, labelled_file), \"r\")\n",
        "      for line in flab:\n",
        "        sentence, label = line.strip().split('\\t')\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "      flab.close()\n",
        "  return sentences, labels\n",
        "\n",
        "sentences, labels = load_sentences_and_labels(\"sentiment labelled sentences\")\n",
        "len(sentences), len(labels)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 3000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNn-z7CDpeo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b4f701-8d5c-49ed-d305-8868fda34aeb"
      },
      "source": [
        "sentences_train, sentences_test, labels_train, labels_test = train_test_split(\n",
        "    sentences, labels, test_size=0.2)\n",
        "\n",
        "len(sentences_train), len(labels_train), len(sentences_test), len(labels_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2400, 2400, 600, 600)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQssnVgg_g8J"
      },
      "source": [
        "### Define and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWOQJxIczjot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "099768c3-f4d2-4ac7-ffc3-e31e1a62c474"
      },
      "source": [
        "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "dev"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abWMWTITqsg3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6589701c-b679-49b8-c6ed-acdf21386039"
      },
      "source": [
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "model = LongformerForSequenceClassification.from_pretrained(\n",
        "    \"allenai/longformer-base-4096\", return_dict=True)\n",
        "\n",
        "# freeze the encoder portion, we just want to update the head weights\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "_ = model.to(dev)\n",
        "_ = model.train()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWwnp3dJsuEV"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr=3e-5)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUxab3wPv76n"
      },
      "source": [
        "NUM_EPOCHS = 2\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLjScgPRvsT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f03f41c-8b9c-4734-c1cf-1984151a1c1d"
      },
      "source": [
        "num_train_sents = len(sentences_train)\n",
        "losses = []\n",
        "for e in range(NUM_EPOCHS):\n",
        "\n",
        "  # shuffle the dataset\n",
        "  permutation = np.random.permutation(len(sentences_train))\n",
        "\n",
        "  for i in range(0, num_train_sents, BATCH_SIZE):\n",
        "\n",
        "    indices = set(permutation[i : i + BATCH_SIZE].tolist())\n",
        "    sentences_batch = [s for i, s in enumerate(sentences_train) if i in indices]\n",
        "    labels_batch = [int(lb) for i, lb in enumerate(labels_train) if i in indices]\n",
        "\n",
        "    encoding = tokenizer(sentences_batch, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(dev)\n",
        "    attention_mask = encoding['attention_mask'].to(dev)\n",
        "    labels = torch.tensor(labels_batch).unsqueeze(0).to(dev)\n",
        "\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    loss = outputs.loss\n",
        "\n",
        "    loss_numpy = loss.item()\n",
        "    losses.append(loss_numpy)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  print(\"EPOCH {:d} | LOSS {:.3f}\".format(e, loss_numpy))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0 | LOSS 0.701\n",
            "EPOCH 1 | LOSS 0.676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UevmNiR_Vwy"
      },
      "source": [
        "### Predict using Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdv36YAr6fwf"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "num_test_sents = len(sentences_test)\n",
        "predictions = []\n",
        "for i in range(0, num_test_sents, BATCH_SIZE):\n",
        "  sentences_batch = sentences_test[i : i + BATCH_SIZE]\n",
        "  encoding = tokenizer(sentences_batch, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids = encoding['input_ids'].to(dev)\n",
        "  attention_mask = encoding['attention_mask'].to(dev)\n",
        "\n",
        "  preds_batch = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "  predictions.extend([0 if lg[0] > lg[1] else 1 \n",
        "    for lg in torch.exp(preds_batch.logits).detach().cpu().numpy().tolist()])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMLD2xfd_Z_8"
      },
      "source": [
        "### Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6JZI83v-iDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed30ca1e-b4b5-41fa-95f8-4d2d16152e68"
      },
      "source": [
        "labels = [int(lb) for lb in labels_test]\n",
        "print(\"accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
        "print(\"confusion matrix\")\n",
        "print(confusion_matrix(labels, predictions))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.807\n",
            "confusion matrix\n",
            "[[262  54]\n",
            " [ 62 222]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWyI_16P_Qxh"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}