{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 01_01_logistic_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcdImGuowOETTJvOrA6GUK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujitpal/nlp-deeplearning-ai-examples/blob/master/01_01_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB0vHNsab8uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5lZEU1EbTPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a6d0989e-de5f-407e-c74c-2b55838899b0"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive # import drive from google colab\n",
        "\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgP155TibnS0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a59b5f75-dcd5-46c1-ccdc-558d9cd839f1"
      },
      "source": [
        "%ls \"drive/My Drive/nlp-deeplearning-ai-data\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentiment-01.pt                 training.1600000.processed.noemoticon.csv\n",
            "testdata.manual.2009.06.14.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oinND4QabxKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = \"drive/My Drive/nlp-deeplearning-ai-data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6U7E_D_b1Ne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4f7c72f3-d0f8-4a99-9fc6-bd0befa55dec"
      },
      "source": [
        "train_df = pd.read_csv(os.path.join(DATA_DIR, \"training.1600000.processed.noemoticon.csv\"), \n",
        "                       names=[\"target\", \"tid\", \"tdate\", \"flag\", \"user\", \"text\"],\n",
        "                       encoding=\"latin1\")\n",
        "# train_df = train_df.sample(frac=0.001)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>tid</th>\n",
              "      <th>tdate</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target  ...                                               text\n",
              "0       0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1       0  ...  is upset that he can't update his Facebook by ...\n",
              "2       0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3       0  ...    my whole body feels itchy and like its on fire \n",
              "4       0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unl2xLsSb4yD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "22f1892d-bb88-4163-bda4-4f634be19ac2"
      },
      "source": [
        "test_df = pd.read_csv(os.path.join(DATA_DIR, \"testdata.manual.2009.06.14.csv\"),\n",
        "                      names=[\"target\", \"tid\", \"tdate\", \"flag\", \"user\", \"text\"])\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>tid</th>\n",
              "      <th>tdate</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>tpryan</td>\n",
              "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>vcu451</td>\n",
              "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>chadfu</td>\n",
              "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>SIX15</td>\n",
              "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>yamarama</td>\n",
              "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target  tid  ...      user                                               text\n",
              "0       4    3  ...    tpryan  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
              "1       4    4  ...    vcu451  Reading my kindle2...  Love it... Lee childs i...\n",
              "2       4    5  ...    chadfu  Ok, first assesment of the #kindle2 ...it fuck...\n",
              "3       4    6  ...     SIX15  @kenburbary You'll love your Kindle2. I've had...\n",
              "4       4    7  ...  yamarama  @mikefish  Fair enough. But i have the Kindle2...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_6WuTbucDNb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec4f5d6a-ce35-4ae8-b32b-8a6aa548e9f9"
      },
      "source": [
        "def preprocess_label(i):\n",
        "  return [1, 0] if i <= 2 else [0, 1]\n",
        "\n",
        "def preprocess_text(s):\n",
        "  s = s.lower()\n",
        "  toks = s.split()\n",
        "  toks = [t for t in toks if not(t.startswith(\"@\"))]\n",
        "  return \" \".join(toks)\n",
        "\n",
        "texts_trainval = [preprocess_text(t) for t in train_df[\"text\"].values]\n",
        "labels_trainval = [preprocess_label(i) for i in train_df[\"target\"].values]\n",
        "\n",
        "texts_train, texts_val, labels_train, labels_val = train_test_split(\n",
        "    texts_trainval, labels_trainval, test_size=0.2)\n",
        "\n",
        "texts_test = [preprocess_text(t) for t in test_df[\"text\"].values]\n",
        "labels_test = [preprocess_label(i) for i in test_df[\"target\"].values]\n",
        "\n",
        "print(len(texts_train), len(labels_train), len(texts_val), \n",
        "      len(labels_val), len(texts_test), len(labels_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1280000 1280000 320000 320000 498 498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVAWegfqdL5f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c479d474-3312-4c93-981b-c9a0dd3b20cc"
      },
      "source": [
        "vectorizer = CountVectorizer(min_df=5, max_features=5000)\n",
        "Xtrain = vectorizer.fit_transform(texts_train)\n",
        "Xval = vectorizer.transform(texts_val)\n",
        "Xtest = vectorizer.transform(texts_test)\n",
        "\n",
        "Xtrain.shape, Xval.shape, Xtest.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1280000, 5000), (320000, 5000), (498, 5000))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AWD1zKCd-8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentDataset(Dataset):\n",
        "  def __init__(self, X, y=None):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    if self.y is not None:\n",
        "      return torch.tensor(self.X[i].toarray(), dtype=torch.float32), \\\n",
        "             torch.tensor(np.array(self.y[i]), dtype=torch.float32)\n",
        "    else:\n",
        "      return torch.tensor(self.X[i].toarray(), dtype=torch.float32), None\n",
        "\n",
        "train_ds = SentimentDataset(Xtrain, np.array(labels_train))\n",
        "val_ds = SentimentDataset(Xval, np.array(labels_val))\n",
        "test_ds = SentimentDataset(Xtest, np.array(labels_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLHZBv57hW-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
        "test_dl = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQrjZdp_iVf9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ed077ce8-9104-4786-d827-88e789d0d0e8"
      },
      "source": [
        "class SentimentLogisticNet(torch.nn.Module):\n",
        "  def __init__(self, vocab_size, output_size):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(vocab_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    x = F.softmax(x)\n",
        "    return x\n",
        "\n",
        "net = SentimentLogisticNet(vocab_size=len(vectorizer.vocabulary_),\n",
        "                           output_size=2) \n",
        "net"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentimentLogisticNet(\n",
              "  (linear): Linear(in_features=5000, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miMewAgGkNKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, dev, train_dl, val_dl, num_epochs=10, lr=1e-3):\n",
        "  params = filter(lambda p: p.requires_grad, net.parameters())\n",
        "  optimizer = torch.optim.Adam(params, lr=lr)\n",
        "  for i in range(num_epochs):\n",
        "    net.train()\n",
        "    sum_loss, total = 0, 0\n",
        "    for x, y in train_dl:\n",
        "      x, y = x.to(dev), y.to(dev)\n",
        "      y_ = net(x)\n",
        "      optimizer.zero_grad()\n",
        "      loss = F.binary_cross_entropy(y_, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      sum_loss += loss.item() * y.shape[0]\n",
        "      total += y.shape[0]\n",
        "    val_loss, val_acc = evaluate(net, dev, val_dl)\n",
        "    print(\"EPOCH {:d}: train loss: {:.3f}, val loss: {:.3f}, val acc: {:.3f}\"\n",
        "      .format(i, sum_loss / total, val_loss, val_acc))\n",
        "\n",
        "\n",
        "def evaluate(net, dev, val_dl):\n",
        "  net.eval()\n",
        "  correct, total, sum_loss = 0, 0, 0\n",
        "  for x, y in val_dl:\n",
        "    x, y = x.to(dev), y.to(dev)\n",
        "    y_ = net(x)\n",
        "    loss = F.binary_cross_entropy(y_, y)\n",
        "    _, pred = torch.max(y_, 1)\n",
        "    correct += (pred == y).float().sum()\n",
        "    total += y.shape[0]\n",
        "    sum_loss += loss.item() * y.shape[0]\n",
        "  return sum_loss / total, correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lVgOi-UlaFP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "281ea2fa-669e-4d2e-bd07-c1072662b3d0"
      },
      "source": [
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(dev)\n",
        "\n",
        "train(net, dev, train_dl, val_dl, num_epochs=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Using a target size (torch.Size([32, 2])) that is different to the input size (torch.Size([32, 1, 2])) is deprecated. Please ensure they have the same size.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Using a target size (torch.Size([32, 2])) that is different to the input size (torch.Size([32, 1, 2])) is deprecated. Please ensure they have the same size.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0: train loss: 1.749, val loss: 1.749, val acc: 1.000\n",
            "EPOCH 1: train loss: 1.749, val loss: 1.749, val acc: 1.000\n",
            "EPOCH 2: train loss: 1.749, val loss: 1.749, val acc: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qanghTLKlka4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e98ccf7d-948c-453d-fb35-fb6f7b2900ff"
      },
      "source": [
        "test_loss, test_acc = evaluate(net, dev, test_dl)\n",
        "print(\"test loss: {:.3f}, test acc: {:.3f}\".format(test_loss, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss: 1.739, test acc: 1.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Using a target size (torch.Size([32, 2])) that is different to the input size (torch.Size([32, 1, 2])) is deprecated. Please ensure they have the same size.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Using a target size (torch.Size([18, 2])) that is different to the input size (torch.Size([18, 1, 2])) is deprecated. Please ensure they have the same size.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc5wo0lnCtLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}