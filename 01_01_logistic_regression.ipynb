{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-01-logistic-regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOECttB5qhQ6J1ZfUNM+wEt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujitpal/nlp-deeplearning-ai-examples/blob/master/01_01_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eSZclmjcCyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G36VzN0_kmMT",
        "colab_type": "text"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJAGK6l1apn0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "fca7e8dd-4031-4d4a-a680-4b377d3cb984"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive # import drive from google colab\n",
        "\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb_9DkvocBri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1efd0b88-a79c-4bf0-f73f-554ec1ab3f9e"
      },
      "source": [
        "%ls \"drive/My Drive/nlp-deeplearning-ai-data\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testdata.manual.2009.06.14.csv  training.1600000.processed.noemoticon.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT3_YaAtmncA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %rm \"drive/My Drive/nlp-deeplearning-ai-data/vocab.pkl\"\n",
        "# %rm \"drive/My Drive/nlp-deeplearning-ai-data/index2word.pkl\"\n",
        "# %rm \"drive/My Drive/nlp-deeplearning-ai-data/word2index.pkl\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U9WOtXfdz4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = \"drive/My Drive/nlp-deeplearning-ai-data\"\n",
        "CUDA_LAUNCH_BLOCKING=\"1\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrH7qEXCgXnl",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbR1bmmteBPn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ccf2f687-854b-40ce-bcfc-7197f226086d"
      },
      "source": [
        "train_df = pd.read_csv(os.path.join(DATA_DIR, \"training.1600000.processed.noemoticon.csv\"), \n",
        "                       names=[\"target\", \"tid\", \"tdate\", \"flag\", \"user\", \"text\"],\n",
        "                       encoding=\"latin1\")\n",
        "train_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>tid</th>\n",
              "      <th>tdate</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target  ...                                               text\n",
              "0       0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1       0  ...  is upset that he can't update his Facebook by ...\n",
              "2       0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3       0  ...    my whole body feels itchy and like its on fire \n",
              "4       0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS5N7EvYdi2D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8d681189-1f3f-4c6e-dd00-e752250e785e"
      },
      "source": [
        "test_df = pd.read_csv(os.path.join(DATA_DIR, \"testdata.manual.2009.06.14.csv\"),\n",
        "                      names=[\"target\", \"tid\", \"tdate\", \"flag\", \"user\", \"text\"])\n",
        "test_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>tid</th>\n",
              "      <th>tdate</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>tpryan</td>\n",
              "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>vcu451</td>\n",
              "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>chadfu</td>\n",
              "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>SIX15</td>\n",
              "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>yamarama</td>\n",
              "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target  tid  ...      user                                               text\n",
              "0       4    3  ...    tpryan  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
              "1       4    4  ...    vcu451  Reading my kindle2...  Love it... Lee childs i...\n",
              "2       4    5  ...    chadfu  Ok, first assesment of the #kindle2 ...it fuck...\n",
              "3       4    6  ...     SIX15  @kenburbary You'll love your Kindle2. I've had...\n",
              "4       4    7  ...  yamarama  @mikefish  Fair enough. But i have the Kindle2...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvPjrfIw09UV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4d41abd-d384-48d6-ea43-4064390bdf47"
      },
      "source": [
        "test_df[\"target\"].unique()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 0, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95hyyr7np4nF",
        "colab_type": "text"
      },
      "source": [
        "### Decide thresholds\n",
        "\n",
        "These are one-time computations needed to determine thresholds\n",
        "* __vocabulary size__ : vocabulary is composed of the most frequent words used in training set, words outside this vocabulary will be marked as UNK.\n",
        "* __maximum length of input sequence__ : we want to figure out the distribution of input sentence lengths (in tokens) so we have an optimum max length. Sentences longer than this length will be truncated, and sentences shorter than this will be PADded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzBZtotZqYp0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d6f9185f-7644-46fe-fd64-b50754671d48"
      },
      "source": [
        "vocab_file = os.path.join(DATA_DIR, \"vocab.pkl\")\n",
        "if not os.path.exists(vocab_file):\n",
        "  num_recs = len(train_df)\n",
        "  word_counts = collections.Counter()\n",
        "  for i, text in enumerate(train_df[\"text\"].values):\n",
        "    if i % (num_recs // 100) == 0:\n",
        "      print(\".\", end=\"\")\n",
        "    text = text.lower()\n",
        "    for token in text.split():\n",
        "      if token.startswith(\"@\"):\n",
        "        continue\n",
        "      word_counts[token] += 1\n",
        "  print(\".\")\n",
        "\n",
        "len([w for w, c in word_counts.most_common() if c > 10])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....................................................................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51145"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07A34-7up26P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8c923b5-76e4-4efb-ac40-ecaf84b31150"
      },
      "source": [
        "num_tokens = []\n",
        "for text in train_df.text.values:\n",
        "  num_tokens.append(len(text.split()))\n",
        "\n",
        "np.percentile(np.array(num_tokens), np.array([75, 80, 90, 95, 99]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([19., 20., 23., 25., 28.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6cONwvFxk_F",
        "colab_type": "text"
      },
      "source": [
        "### Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMArY5HJOhIp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4d7b6fa-37f8-4adb-fa16-f0980cc4e926"
      },
      "source": [
        "# train_df = train_df.sample(frac=0.001)\n",
        "texts = train_df[\"text\"].values\n",
        "labels = train_df[\"target\"].values\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2)\n",
        "len(train_texts), len(train_labels), len(val_texts), len(val_labels)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1280000, 1280000, 320000, 320000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27S72PAgduKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c26c4aa8-36f9-42b5-c94b-aa12a985494c"
      },
      "source": [
        "class SentimentDataset(Dataset):\n",
        "  def __init__(self, texts, labels=None, vocab=None, \n",
        "               vocab_size=50000, maxlen=30):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    if vocab is None:\n",
        "      self.vocab_ = self._build_vocab(texts, vocab_size)\n",
        "    else:\n",
        "      self.vocab_ = vocab\n",
        "    self.word2index_ = {w:i+2 for i, w in enumerate(self.vocab_)}\n",
        "    self.word2index_[\"PAD\"] = 0\n",
        "    self.word2index_[\"UNK\"] = 1\n",
        "    self.index2word_ = {v:k for k, v in self.word2index_.items()}\n",
        "    self.vocab_size = vocab_size\n",
        "    self.maxlen = maxlen\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    tokens = []\n",
        "    for token in self.texts[idx].split():\n",
        "      if token in self.word2index_.keys():\n",
        "        tokens.append(self.word2index_[token])\n",
        "      else:\n",
        "        tokens.append(self.word2index_[\"UNK\"])\n",
        "    if len(tokens) > self.maxlen:\n",
        "      tokens = tokens[0:self.maxlen]\n",
        "    elif len(tokens) < self.maxlen:\n",
        "      tokens = [self.word2index_[\"PAD\"]] * (self.maxlen - len(tokens)) + tokens\n",
        "    else:\n",
        "      pass\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "    if self.labels is not None:\n",
        "      label = self.labels[idx]\n",
        "      label = 1 if label == 4 else 0\n",
        "      return (tokens, label)\n",
        "    else:\n",
        "      return tokens\n",
        "\n",
        "  def _build_vocab(self, texts, vocab_size):\n",
        "    num_recs = len(texts)\n",
        "    word_counts = collections.Counter()\n",
        "    for i, text in enumerate(texts):\n",
        "      if i % (num_recs // 100) == 0:\n",
        "        print(\".\", end=\"\")\n",
        "      text = text.lower()\n",
        "      for token in text.split():\n",
        "        if token.startswith(\"@\"):\n",
        "          continue\n",
        "        word_counts[token] += 1\n",
        "    # truncate to vocab_size\n",
        "    vocab = [w for w, c in word_counts.most_common(vocab_size)]\n",
        "    vocab.append(\"PAD\")\n",
        "    vocab.append(\"UNK\")\n",
        "    return vocab\n",
        "\n",
        "train_dataset = SentimentDataset(train_texts, train_labels)\n",
        "val_dataset = SentimentDataset(val_texts, val_labels, \n",
        "                               vocab=train_dataset.vocab_)\n",
        "test_dataset = SentimentDataset(test_df[\"text\"].values,\n",
        "                                labels=test_df[\"target\"].values,\n",
        "                                vocab=train_dataset.vocab_)\n",
        "train_dataset[10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...................................................................................................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             1,     1,    75,   137,  1734,   125,  1615,     4,   175,   467,\n",
              "             1,    12,   543,   102,     7,     8,    55,    28,    62, 47222]),\n",
              " 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReuST19GOyRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUPrF1Bo8R-w",
        "colab_type": "text"
      },
      "source": [
        "### Network Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2t_Rfeazzwy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8e7ef9d7-1970-4b8d-9d46-de607975eb7f"
      },
      "source": [
        "class SentimentNet(torch.nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_dim, num_targets):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "    self.linear = nn.Linear(hidden_dim, num_targets)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.dropout(x)\n",
        "    lstm_out, (ht, ct) = self.lstm(x)\n",
        "    return self.linear(ht[-1])\n",
        "\n",
        "\n",
        "net = SentimentNet(vocab_size=len(train_dataset.vocab_), \n",
        "                   embed_dim=128,\n",
        "                   hidden_dim=50,\n",
        "                   num_targets=2)\n",
        "net"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentimentNet(\n",
              "  (embedding): Embedding(50002, 128, padding_idx=0)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (lstm): LSTM(128, 50, batch_first=True)\n",
              "  (linear): Linear(in_features=50, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPmxOmVF8a-0",
        "colab_type": "text"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FVLonAt8NIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, train_dataloader, val_dataloader, dev, num_epochs=10, lr=0.001):\n",
        "  params = filter(lambda p: p.requires_grad, net.parameters())\n",
        "  optimizer = torch.optim.Adam(params, lr=lr)\n",
        "  for i in range(num_epochs):\n",
        "    net.train()\n",
        "    sum_loss, total = 0, 0\n",
        "    for x, y in train_dataloader:\n",
        "      x, y = x.to(dev), y.to(dev)\n",
        "      # print(x.size(), y.size())\n",
        "      y_ = net(x)\n",
        "      optimizer.zero_grad()\n",
        "      loss = F.cross_entropy(y_, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      sum_loss += loss.item() * y.shape[0]\n",
        "      total += y.shape[0]\n",
        "    val_loss, val_acc = evaluate(net, val_dataloader, dev)\n",
        "    print(\"EPOCH {:d}: train loss: {:.3f}, val loss: {:.3f}, val acc: {:.3f}\"\n",
        "      .format(i, sum_loss / total, val_loss, val_acc))\n",
        "    \n",
        "\n",
        "def evaluate(net, val_dataloader, dev):\n",
        "  net.eval()\n",
        "  correct, total, sum_loss = 0, 0, 0\n",
        "  for x, y in val_dataloader:\n",
        "    x, y = x.to(dev), y.to(dev)\n",
        "    y_ = net(x)\n",
        "    loss = F.cross_entropy(y_, y)\n",
        "    _, pred = torch.max(y_, 1)\n",
        "    correct += (pred == y).float().sum()\n",
        "    total += y.shape[0]\n",
        "    sum_loss += loss.item() * y.shape[0]\n",
        "  return sum_loss / total, correct / total"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQGxw9ZMSEkO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ea4661c5-a6ff-4ada-95f9-672f0f5ea5e6"
      },
      "source": [
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# dev = torch.device(\"cpu\")\n",
        "net.to(dev)\n",
        "\n",
        "train(net, train_dataloader=train_dataloader, val_dataloader=val_dataloader, dev=dev)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0: train loss: 0.486, val loss: 0.445, val acc: 0.791\n",
            "EPOCH 1: train loss: 0.438, val loss: 0.435, val acc: 0.797\n",
            "EPOCH 2: train loss: 0.423, val loss: 0.432, val acc: 0.800\n",
            "EPOCH 3: train loss: 0.412, val loss: 0.429, val acc: 0.801\n",
            "EPOCH 4: train loss: 0.404, val loss: 0.431, val acc: 0.802\n",
            "EPOCH 5: train loss: 0.398, val loss: 0.432, val acc: 0.802\n",
            "EPOCH 6: train loss: 0.393, val loss: 0.434, val acc: 0.802\n",
            "EPOCH 7: train loss: 0.388, val loss: 0.430, val acc: 0.802\n",
            "EPOCH 8: train loss: 0.385, val loss: 0.435, val acc: 0.802\n",
            "EPOCH 9: train loss: 0.381, val loss: 0.436, val acc: 0.803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTLvmRDuhopz",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate on Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbv-kElYVny4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7800a184-ad57-46bd-8bbe-9e0e7a10a4a3"
      },
      "source": [
        "test_loss, test_acc = evaluate(net, test_dataloader, dev)\n",
        "print(\"test_loss: {:.3f}, test_acc: {:.3f}\".format(test_loss, test_acc))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_loss: 0.819, test_acc: 0.627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_6n9QQgg5Ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(net.state_dict(), os.path.join(DATA_DIR, \"sentiment-01.pt\"))\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM18niBu07Ak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5de81126-2124-431e-fa9e-27ac5daadba8"
      },
      "source": [
        "%ls \"drive/My Drive/nlp-deeplearning-ai-data\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentiment-01.pt                 training.1600000.processed.noemoticon.csv\n",
            "testdata.manual.2009.06.14.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE8TaCbt1AfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}